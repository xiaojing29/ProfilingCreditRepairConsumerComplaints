{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use STSB-BERT model and logistic regression in PyTorch with manual hyperparameter tuning (2 classes)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom Logistic Regression model in PyTorch\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Setup\n",
    "tokenizer = BertTokenizer.from_pretrained('sentence-transformers/stsb-bert-base')\n",
    "bert_model = BertModel.from_pretrained('sentence-transformers/stsb-bert-base')\n",
    "bert_model.eval()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-2-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), torch.tensor(train_labels.values, dtype=torch.float32).view(-1, 1))\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), torch.tensor(test_labels.values, dtype=torch.float32).view(-1, 1))\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "step_size = 30 # Number of epochs after which to reduce learning rate\n",
    "gamma = 0.1 # Reduction factor for learning rate\n",
    "\n",
    "# DataLoader with batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Logistic Regression Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features, labels\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features).view(-1)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    with torch.no_grad():\n",
    "        val_features, val_labels = test_dataset.tensors\n",
    "        val_features, val_labels = val_features, val_labels\n",
    "        val_outputs = model(val_features).view(-1)\n",
    "        val_outputs = torch.sigmoid(val_outputs)\n",
    "        val_loss = criterion(val_outputs, val_labels.view(-1))\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/100], Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break       \n",
    "        \n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(test_features, dtype=torch.float32)).sigmoid().round()\n",
    "    accuracy = accuracy_score(test_labels, predictions.cpu().numpy())\n",
    "    f1 = f1_score(test_labels, predictions.cpu().numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 2 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 2 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd204520",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use STSB-BERT model and logistic regression in PyTorch with manual hyperparameter tuning (4 classes)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom Logistic Regression model in PyTorch\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=4):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Setup for stsb-bert-base model\n",
    "tokenizer = BertTokenizer.from_pretrained('sentence-transformers/stsb-bert-base')\n",
    "bert_model = BertModel.from_pretrained('sentence-transformers/stsb-bert-base')\n",
    "bert_model.eval()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert labels for multi-class classification\n",
    "train_labels = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels.values, dtype=torch.long)\n",
    "\n",
    "# Convert to PyTorch datasets for multi-class\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels)\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "step_size = 30 # Number of epochs after which to reduce learning rate\n",
    "gamma = 0.1 # Reduction factor for learning rate\n",
    "\n",
    "# DataLoader with batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Logistic Regression Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_features, val_labels = test_dataset.tensors\n",
    "        val_outputs = model(val_features)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/500], Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break       \n",
    "\n",
    "# Evaluate the model for multi-class classification\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(test_features, dtype=torch.float32))\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    accuracy = accuracy_score(test_labels.cpu().numpy(), predicted_classes.cpu().numpy())\n",
    "    f1 = f1_score(test_labels.cpu().numpy(), predicted_classes.cpu().numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 4 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 4 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388feddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over 2 classes: 0.8636363636363636\n",
      "F1 Score over 2 classes: 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "### Sentence Transformer with MLPClassifier for 2 classes\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-2-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup for stsb-bert-base model\n",
    "tokenizer = BertTokenizer.from_pretrained('sentence-transformers/stsb-bert-base')\n",
    "bert_model = BertModel.from_pretrained('sentence-transformers/stsb-bert-base')\n",
    "bert_model.eval()\n",
    "\n",
    "# Extract features\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert labels for multi-class classification\n",
    "train_labels = train_labels.values\n",
    "test_labels = test_labels.values\n",
    "\n",
    "# Define MLPClassifier\n",
    "mlp_classifier = MLPClassifier()\n",
    "\n",
    "# Define hyperparameters grid for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 3000, 5000]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(mlp_classifier, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train MLPClassifier with best hyperparameters\n",
    "best_mlp_classifier = MLPClassifier(**best_params)\n",
    "best_mlp_classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Predict using best model\n",
    "predictions = best_mlp_classifier.predict(test_features)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 2 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 2 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a577ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over 4 classes: 0.6363636363636364\n",
      "F1 Score over 4 classes: 0.6341329258976318\n"
     ]
    }
   ],
   "source": [
    "### Sentence Transformer with MLPClassifier for 4 classes\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup for stsb-bert-base model\n",
    "tokenizer = BertTokenizer.from_pretrained('sentence-transformers/stsb-bert-base')\n",
    "bert_model = BertModel.from_pretrained('sentence-transformers/stsb-bert-base')\n",
    "bert_model.eval()\n",
    "\n",
    "# Extract features\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert labels for multi-class classification\n",
    "train_labels = train_labels.values\n",
    "test_labels = test_labels.values\n",
    "\n",
    "# Define MLPClassifier\n",
    "mlp_classifier = MLPClassifier()\n",
    "\n",
    "# Define hyperparameters grid for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 3000, 6000]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(mlp_classifier, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train MLPClassifier with best hyperparameters\n",
    "best_mlp_classifier = MLPClassifier(**best_params)\n",
    "best_mlp_classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Predict using best model\n",
    "predictions = best_mlp_classifier.predict(test_features)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 4 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 4 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fea22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
