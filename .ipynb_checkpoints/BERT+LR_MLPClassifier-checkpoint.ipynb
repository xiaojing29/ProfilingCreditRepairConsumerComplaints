{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7703df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.1, 'solver': 'lbfgs'}\n",
      "Accuracy over 2 classes: 0.9090909090909091\n",
      "F1 Score over 2 classes: 0.9079425837320575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Using Bert and SK-Learn Logistic Regression for 2 classes with grid search\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()  # Extract the [CLS] token's embeddings\n",
    "\n",
    "# Load pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'complaints-official-2-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, model)\n",
    "\n",
    "# Logistic Regression Classifier with Grid Search\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'newton-cg', 'sag', 'saga']\n",
    "}\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_model.predict(test_features)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_2_classes = accuracy_score(test_labels, predictions)\n",
    "f1_2_classes = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Accuracy over 2 classes: {accuracy_2_classes}\")\n",
    "print(f\"F1 Score over 2 classes: {f1_2_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7682d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/scipy/optimize/_linesearch.py:306: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/xiaojingzhang/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'solver': 'lbfgs'}\n",
      "Accuracy over 4 classes: 0.5681818181818182\n",
      "F1 Score over 4 classes: 0.5341991341991342\n"
     ]
    }
   ],
   "source": [
    "### Using Bert and SK-Learn Logistic Regression for 4 classes with grid search\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()  # Extract the [CLS] token's embeddings\n",
    "\n",
    "# Load pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, model)\n",
    "\n",
    "# Logistic Regression Classifier with Grid Search\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'newton-cg', 'sag', 'saga']\n",
    "}\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_model.predict(test_features)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_4_classes = accuracy_score(test_labels, predictions)\n",
    "f1_4_classes = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Accuracy over 4 classes: {accuracy_4_classes}\")\n",
    "print(f\"F1 Score over 4 classes: {f1_4_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57455f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of classes\n",
    "classes = [2, 4]\n",
    "\n",
    "# Accuracies and F1 scores\n",
    "accuracies = [accuracy_2_classes, accuracy_4_classes]\n",
    "f1_scores = [f1_2_classes, f1_4_classes]\n",
    "\n",
    "# Plotting Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(classes, accuracies, marker='o')\n",
    "plt.title('Accuracy over Number of Classes')\n",
    "plt.xlabel('Number of Classes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(classes)\n",
    "\n",
    "# Plotting F1 Score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(classes, f1_scores, marker='o', color='orange')\n",
    "plt.title('F1 Score over Number of Classes')\n",
    "plt.xlabel('Number of Classes')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(classes)\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76eb720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.5644518733024597, Validation Loss: 0.5843958258628845\n",
      "Epoch [2/500], Loss: 0.33395668864250183, Validation Loss: 0.4214724600315094\n",
      "Epoch [3/500], Loss: 0.26291587948799133, Validation Loss: 0.36222484707832336\n",
      "Epoch [4/500], Loss: 0.2608507573604584, Validation Loss: 0.3445083796977997\n",
      "Epoch [5/500], Loss: 0.4415854811668396, Validation Loss: 0.335126668214798\n",
      "Epoch [6/500], Loss: 0.3741483688354492, Validation Loss: 0.33086061477661133\n",
      "Epoch [7/500], Loss: 0.4030067026615143, Validation Loss: 0.32343220710754395\n",
      "Epoch [8/500], Loss: 0.19825170934200287, Validation Loss: 0.3323352634906769\n",
      "Epoch [9/500], Loss: 0.293386310338974, Validation Loss: 0.3114664852619171\n",
      "Epoch [10/500], Loss: 0.2138824760913849, Validation Loss: 0.31846240162849426\n",
      "Epoch [11/500], Loss: 0.15007570385932922, Validation Loss: 0.31408894062042236\n",
      "Epoch [12/500], Loss: 0.2082296758890152, Validation Loss: 0.31197917461395264\n",
      "Epoch [13/500], Loss: 0.15714822709560394, Validation Loss: 0.3129504323005676\n",
      "Early stopping triggered\n",
      "Accuracy over 2 classes: 0.9090909090909091\n",
      "F1 Score over 2 classes: 0.9079425837320575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/1622109090.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels)\n",
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/1622109090.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels)\n",
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/1622109090.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions = model(torch.tensor(test_features, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "### Use Bert base model and logistic regression in PyTorch with manual hyperparameter tuning (2 classes)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom Logistic Regression model in PyTorch\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=4):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Setup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-2-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert labels for multi-class classification\n",
    "train_labels = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels.values, dtype=torch.long)\n",
    "\n",
    "# Convert to PyTorch datasets for multi-class\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels)\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "step_size = 30 # Number of epochs after which to reduce learning rate\n",
    "gamma = 0.1 # Reduction factor for learning rate\n",
    "\n",
    "# DataLoader with batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Logistic Regression Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_features, val_labels = test_dataset.tensors\n",
    "        val_outputs = model(val_features)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/500], Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break       \n",
    "\n",
    "# Evaluate the model for multi-class classification\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(test_features, dtype=torch.float32))\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    accuracy = accuracy_score(test_labels.cpu().numpy(), predicted_classes.cpu().numpy())\n",
    "    f1 = f1_score(test_labels.cpu().numpy(), predicted_classes.cpu().numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 2 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 2 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a6e680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 1.207716464996338, Validation Loss: 1.2126336097717285\n",
      "Epoch [2/500], Loss: 0.9372875690460205, Validation Loss: 1.0705554485321045\n",
      "Epoch [3/500], Loss: 0.9650571346282959, Validation Loss: 1.0152121782302856\n",
      "Epoch [4/500], Loss: 0.6957279443740845, Validation Loss: 0.9959644675254822\n",
      "Epoch [5/500], Loss: 0.8556903004646301, Validation Loss: 0.9856973886489868\n",
      "Epoch [6/500], Loss: 0.7938104867935181, Validation Loss: 0.9832984805107117\n",
      "Epoch [7/500], Loss: 0.8088856339454651, Validation Loss: 0.9693134427070618\n",
      "Epoch [8/500], Loss: 0.8243502378463745, Validation Loss: 0.9671409726142883\n",
      "Epoch [9/500], Loss: 0.8722999691963196, Validation Loss: 0.9729942083358765\n",
      "Epoch [10/500], Loss: 0.6103265881538391, Validation Loss: 0.9550302624702454\n",
      "Epoch [11/500], Loss: 0.5601005554199219, Validation Loss: 0.9751741290092468\n",
      "Epoch [12/500], Loss: 0.5537860989570618, Validation Loss: 0.9685116410255432\n",
      "Epoch [13/500], Loss: 0.7330509424209595, Validation Loss: 0.9663064479827881\n",
      "Epoch [14/500], Loss: 0.5064991116523743, Validation Loss: 0.984580934047699\n",
      "Early stopping triggered\n",
      "Accuracy over 4 classes: 0.6136363636363636\n",
      "F1 Score over 4 classes: 0.5890392771528675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/2562977968.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels)\n",
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/2562977968.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels)\n",
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/2562977968.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions = model(torch.tensor(test_features, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "### Use Bert base model and logistic regression in PyTorch with manual hyperparameter tuning (4 classes)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom Logistic Regression model in PyTorch\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=4):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Setup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert labels for multi-class classification\n",
    "train_labels = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels.values, dtype=torch.long)\n",
    "\n",
    "# Convert to PyTorch datasets for multi-class\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels)\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "step_size = 30 # Number of epochs after which to reduce learning rate\n",
    "gamma = 0.1 # Reduction factor for learning rate\n",
    "\n",
    "# DataLoader with batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Logistic Regression Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_features, val_labels = test_dataset.tensors\n",
    "        val_outputs = model(val_features)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/500], Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break       \n",
    "\n",
    "# Evaluate the model for multi-class classification\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(test_features, dtype=torch.float32))\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    accuracy = accuracy_score(test_labels.cpu().numpy(), predicted_classes.cpu().numpy())\n",
    "    f1 = f1_score(test_labels.cpu().numpy(), predicted_classes.cpu().numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 4 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 4 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8e73dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/437078878.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels_tensor)\n",
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/437078878.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.5350081920623779, Validation Loss: 0.5554035305976868\n",
      "Epoch [2/500], Loss: 0.5409982204437256, Validation Loss: 0.41329482197761536\n",
      "Epoch [3/500], Loss: 0.2885081470012665, Validation Loss: 0.3692046105861664\n",
      "Epoch [4/500], Loss: 0.3485986292362213, Validation Loss: 0.3448679745197296\n",
      "Epoch [5/500], Loss: 0.4510560631752014, Validation Loss: 0.3441982865333557\n",
      "Epoch [6/500], Loss: 0.26864615082740784, Validation Loss: 0.3384951055049896\n",
      "Epoch [7/500], Loss: 0.12813475728034973, Validation Loss: 0.33773139119148254\n",
      "Epoch [8/500], Loss: 0.26063773036003113, Validation Loss: 0.3333023488521576\n",
      "Epoch [9/500], Loss: 0.2515496611595154, Validation Loss: 0.32108232378959656\n",
      "Epoch [10/500], Loss: 0.12282337993383408, Validation Loss: 0.3270810544490814\n",
      "Epoch [11/500], Loss: 0.3081207275390625, Validation Loss: 0.32891979813575745\n",
      "Epoch [12/500], Loss: 0.4013952612876892, Validation Loss: 0.3148714303970337\n",
      "Epoch [13/500], Loss: 0.148298978805542, Validation Loss: 0.3221301734447479\n",
      "Epoch [14/500], Loss: 0.4445156753063202, Validation Loss: 0.3178865611553192\n",
      "Epoch [15/500], Loss: 0.12608487904071808, Validation Loss: 0.31845468282699585\n",
      "Epoch [16/500], Loss: 0.1780676692724228, Validation Loss: 0.32040631771087646\n",
      "Epoch [17/500], Loss: 0.18785448372364044, Validation Loss: 0.31246617436408997\n",
      "Epoch [18/500], Loss: 0.06948811560869217, Validation Loss: 0.3184930384159088\n",
      "Epoch [19/500], Loss: 0.3180602192878723, Validation Loss: 0.3185648024082184\n",
      "Epoch [20/500], Loss: 0.23091290891170502, Validation Loss: 0.3030124008655548\n",
      "Epoch [21/500], Loss: 0.17135490477085114, Validation Loss: 0.3198666274547577\n",
      "Epoch [22/500], Loss: 0.24001464247703552, Validation Loss: 0.3119003474712372\n",
      "Epoch [23/500], Loss: 0.1403174102306366, Validation Loss: 0.30727851390838623\n",
      "Epoch [24/500], Loss: 0.17409849166870117, Validation Loss: 0.30837076902389526\n",
      "Epoch [25/500], Loss: 0.12344428151845932, Validation Loss: 0.3094874620437622\n",
      "Epoch [26/500], Loss: 0.232750803232193, Validation Loss: 0.3208535313606262\n",
      "Epoch [27/500], Loss: 0.2570044696331024, Validation Loss: 0.29353880882263184\n",
      "Epoch [28/500], Loss: 0.11463109403848648, Validation Loss: 0.3295290172100067\n",
      "Epoch [29/500], Loss: 0.09412030130624771, Validation Loss: 0.30186140537261963\n",
      "Epoch [30/500], Loss: 0.1600906401872635, Validation Loss: 0.31404969096183777\n",
      "Epoch [31/500], Loss: 0.22626273334026337, Validation Loss: 0.31435897946357727\n",
      "Epoch [32/500], Loss: 0.10411030054092407, Validation Loss: 0.3096175789833069\n",
      "Epoch [33/500], Loss: 0.17975294589996338, Validation Loss: 0.30759432911872864\n",
      "Epoch [34/500], Loss: 0.17439372837543488, Validation Loss: 0.3073374629020691\n",
      "Epoch [35/500], Loss: 0.12538371980190277, Validation Loss: 0.30588945746421814\n",
      "Epoch [36/500], Loss: 0.17272822558879852, Validation Loss: 0.30407917499542236\n",
      "Epoch [37/500], Loss: 0.17258374392986298, Validation Loss: 0.30417314171791077\n",
      "Epoch [38/500], Loss: 0.15090136229991913, Validation Loss: 0.3057591915130615\n",
      "Epoch [39/500], Loss: 0.11376936733722687, Validation Loss: 0.3076625168323517\n",
      "Epoch [40/500], Loss: 0.1671082079410553, Validation Loss: 0.30568012595176697\n",
      "Epoch [41/500], Loss: 0.10044465214014053, Validation Loss: 0.3060319423675537\n",
      "Epoch [42/500], Loss: 0.1177004724740982, Validation Loss: 0.30774232745170593\n",
      "Epoch [43/500], Loss: 0.1690710335969925, Validation Loss: 0.30633169412612915\n",
      "Epoch [44/500], Loss: 0.2635957896709442, Validation Loss: 0.3070186674594879\n",
      "Epoch [45/500], Loss: 0.17749443650245667, Validation Loss: 0.3067724108695984\n",
      "Epoch [46/500], Loss: 0.134165421128273, Validation Loss: 0.30711740255355835\n",
      "Epoch [47/500], Loss: 0.17697188258171082, Validation Loss: 0.3066178262233734\n",
      "Epoch [48/500], Loss: 0.16363181173801422, Validation Loss: 0.3051460385322571\n",
      "Epoch [49/500], Loss: 0.1510573923587799, Validation Loss: 0.3042418658733368\n",
      "Epoch [50/500], Loss: 0.12989161908626556, Validation Loss: 0.30467402935028076\n",
      "Epoch [51/500], Loss: 0.19599123299121857, Validation Loss: 0.3048023581504822\n",
      "Epoch [52/500], Loss: 0.23519673943519592, Validation Loss: 0.3072556257247925\n",
      "Epoch [53/500], Loss: 0.06678573042154312, Validation Loss: 0.3054889738559723\n",
      "Epoch [54/500], Loss: 0.18624229729175568, Validation Loss: 0.3046249449253082\n",
      "Epoch [55/500], Loss: 0.14905855059623718, Validation Loss: 0.30526602268218994\n",
      "Epoch [56/500], Loss: 0.059832312166690826, Validation Loss: 0.30466771125793457\n",
      "Epoch [57/500], Loss: 0.29045844078063965, Validation Loss: 0.304656058549881\n",
      "Epoch [58/500], Loss: 0.06076866760849953, Validation Loss: 0.30513596534729004\n",
      "Epoch [59/500], Loss: 0.13002295792102814, Validation Loss: 0.30639389157295227\n",
      "Epoch [60/500], Loss: 0.15090873837471008, Validation Loss: 0.30449768900871277\n",
      "Epoch [61/500], Loss: 0.04581362381577492, Validation Loss: 0.3045263886451721\n",
      "Epoch [62/500], Loss: 0.21512733399868011, Validation Loss: 0.30477455258369446\n",
      "Epoch [63/500], Loss: 0.19416461884975433, Validation Loss: 0.3047402501106262\n",
      "Epoch [64/500], Loss: 0.14058220386505127, Validation Loss: 0.30458730459213257\n",
      "Epoch [65/500], Loss: 0.16291718184947968, Validation Loss: 0.30486997961997986\n",
      "Epoch [66/500], Loss: 0.23219028115272522, Validation Loss: 0.3046817183494568\n",
      "Epoch [67/500], Loss: 0.11721642315387726, Validation Loss: 0.30452340841293335\n",
      "Epoch [68/500], Loss: 0.18513013422489166, Validation Loss: 0.30456405878067017\n",
      "Epoch [69/500], Loss: 0.09722483903169632, Validation Loss: 0.3045315444469452\n",
      "Epoch [70/500], Loss: 0.11577142775058746, Validation Loss: 0.3044055700302124\n",
      "Epoch [71/500], Loss: 0.14200958609580994, Validation Loss: 0.30476051568984985\n",
      "Epoch [72/500], Loss: 0.10038671642541885, Validation Loss: 0.304665744304657\n",
      "Epoch [73/500], Loss: 0.16403360664844513, Validation Loss: 0.304848313331604\n",
      "Epoch [74/500], Loss: 0.1371447890996933, Validation Loss: 0.3045918345451355\n",
      "Epoch [75/500], Loss: 0.06256623566150665, Validation Loss: 0.3046748638153076\n",
      "Epoch [76/500], Loss: 0.10155463963747025, Validation Loss: 0.30488577485084534\n",
      "Epoch [77/500], Loss: 0.036743633449077606, Validation Loss: 0.3048674166202545\n",
      "Epoch [78/500], Loss: 0.15308105945587158, Validation Loss: 0.30501484870910645\n",
      "Early stopping triggered\n",
      "Accuracy over 4 classes: 0.9090909090909091\n",
      "F1 Score over 4 classes: 0.9079425837320575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/437078878.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions = model(torch.tensor(test_features, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "### Use Bert base model and MLPClassifer for 2 classes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Custom MLP Classifier model in PyTorch\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=4):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(512, 128)        # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, num_classes) # Output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here, CrossEntropyLoss will take care of that\n",
    "        return x\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Setup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-2-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.long)\n",
    "\n",
    "# Convert to PyTorch datasets for multi-class\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels_tensor)\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels_tensor)\n",
    "\n",
    "# MLP Classifier Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = MLPClassifier(input_dim)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "step_size = 30 # Number of epochs after which to reduce learning rate\n",
    "gamma = 0.1 # Reduction factor for learning rate\n",
    "\n",
    "# DataLoader with batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Logistic Regression Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_features, val_labels = test_dataset.tensors\n",
    "        val_outputs = model(val_features)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/500], Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break       \n",
    "\n",
    "# Evaluate the model for multi-class classification\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(test_features, dtype=torch.float32))\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    accuracy = accuracy_score(test_labels, predicted_classes)\n",
    "    f1 = f1_score(test_labels, predicted_classes, average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 2 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 2 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c29e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'hidden_layer_sizes': (150,), 'max_iter': 3000}\n",
      "Accuracy over 4 classes: 0.5227272727272727\n",
      "F1 Score over 4 classes: 0.48791920154149254\n"
     ]
    }
   ],
   "source": [
    "### Bert base model with MLPClassifier for 4 classes\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup for stsb-bert-base model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Extract features\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert labels for multi-class classification\n",
    "train_labels = train_labels.values\n",
    "test_labels = test_labels.values\n",
    "\n",
    "# Define MLPClassifier\n",
    "mlp_classifier = MLPClassifier()\n",
    "\n",
    "# Define hyperparameters grid for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 3000, 6000]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(mlp_classifier, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "# Train MLPClassifier with best hyperparameters\n",
    "best_mlp_classifier = MLPClassifier(**best_params)\n",
    "best_mlp_classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Predict using best model\n",
    "predictions = best_mlp_classifier.predict(test_features)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 4 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 4 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cad27f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/3884871855.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels_tensor)\n",
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/3884871855.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels_tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 1.44520103931427, Validation Loss: 1.4954322576522827\n",
      "Epoch [2/500], Loss: 1.5424662828445435, Validation Loss: 1.4854217767715454\n",
      "Epoch [3/500], Loss: 1.505610466003418, Validation Loss: 1.4756115674972534\n",
      "Epoch [4/500], Loss: 1.3877770900726318, Validation Loss: 1.4664219617843628\n",
      "Epoch [5/500], Loss: 1.461161732673645, Validation Loss: 1.4588688611984253\n",
      "Epoch [6/500], Loss: 1.4299789667129517, Validation Loss: 1.4505778551101685\n",
      "Epoch [7/500], Loss: 1.3534594774246216, Validation Loss: 1.4428166151046753\n",
      "Epoch [8/500], Loss: 1.3821220397949219, Validation Loss: 1.435682773590088\n",
      "Epoch [9/500], Loss: 1.539665699005127, Validation Loss: 1.4301848411560059\n",
      "Epoch [10/500], Loss: 1.345664620399475, Validation Loss: 1.4232988357543945\n",
      "Epoch [11/500], Loss: 1.3707780838012695, Validation Loss: 1.4170947074890137\n",
      "Epoch [12/500], Loss: 1.3617295026779175, Validation Loss: 1.411363959312439\n",
      "Epoch [13/500], Loss: 1.3184067010879517, Validation Loss: 1.4055405855178833\n",
      "Epoch [14/500], Loss: 1.2745351791381836, Validation Loss: 1.4003305435180664\n",
      "Epoch [15/500], Loss: 1.3486663103103638, Validation Loss: 1.3956373929977417\n",
      "Epoch [16/500], Loss: 1.3465579748153687, Validation Loss: 1.3904973268508911\n",
      "Epoch [17/500], Loss: 1.3275232315063477, Validation Loss: 1.384596586227417\n",
      "Epoch [18/500], Loss: 1.4472671747207642, Validation Loss: 1.379104495048523\n",
      "Epoch [19/500], Loss: 1.2783222198486328, Validation Loss: 1.3740462064743042\n",
      "Epoch [20/500], Loss: 1.286240577697754, Validation Loss: 1.369335651397705\n",
      "Epoch [21/500], Loss: 1.3047577142715454, Validation Loss: 1.3646304607391357\n",
      "Epoch [22/500], Loss: 1.3198281526565552, Validation Loss: 1.3602973222732544\n",
      "Epoch [23/500], Loss: 1.373268485069275, Validation Loss: 1.355963945388794\n",
      "Epoch [24/500], Loss: 1.2478241920471191, Validation Loss: 1.350767970085144\n",
      "Epoch [25/500], Loss: 1.3419901132583618, Validation Loss: 1.3460692167282104\n",
      "Epoch [26/500], Loss: 1.2551084756851196, Validation Loss: 1.3419028520584106\n",
      "Epoch [27/500], Loss: 1.3796106576919556, Validation Loss: 1.3377922773361206\n",
      "Epoch [28/500], Loss: 1.2630071640014648, Validation Loss: 1.3334780931472778\n",
      "Epoch [29/500], Loss: 1.3076767921447754, Validation Loss: 1.329358458518982\n",
      "Epoch [30/500], Loss: 1.3086795806884766, Validation Loss: 1.325327754020691\n",
      "Epoch [31/500], Loss: 1.2071750164031982, Validation Loss: 1.3248515129089355\n",
      "Epoch [32/500], Loss: 1.1351065635681152, Validation Loss: 1.3243917226791382\n",
      "Epoch [33/500], Loss: 1.3192775249481201, Validation Loss: 1.3240784406661987\n",
      "Epoch [34/500], Loss: 1.2180442810058594, Validation Loss: 1.323626160621643\n",
      "Epoch [35/500], Loss: 1.2423127889633179, Validation Loss: 1.323177456855774\n",
      "Epoch [36/500], Loss: 1.3313145637512207, Validation Loss: 1.322798490524292\n",
      "Epoch [37/500], Loss: 1.205998182296753, Validation Loss: 1.3224139213562012\n",
      "Epoch [38/500], Loss: 1.210634469985962, Validation Loss: 1.3219447135925293\n",
      "Epoch [39/500], Loss: 1.271963357925415, Validation Loss: 1.3215546607971191\n",
      "Epoch [40/500], Loss: 1.3458786010742188, Validation Loss: 1.3212053775787354\n",
      "Epoch [41/500], Loss: 1.2408757209777832, Validation Loss: 1.3207237720489502\n",
      "Epoch [42/500], Loss: 1.3121546506881714, Validation Loss: 1.3203390836715698\n",
      "Epoch [43/500], Loss: 1.266656756401062, Validation Loss: 1.3199191093444824\n",
      "Epoch [44/500], Loss: 1.1986263990402222, Validation Loss: 1.319482445716858\n",
      "Epoch [45/500], Loss: 1.290915608406067, Validation Loss: 1.3190535306930542\n",
      "Epoch [46/500], Loss: 1.1111280918121338, Validation Loss: 1.3186265230178833\n",
      "Epoch [47/500], Loss: 1.2505440711975098, Validation Loss: 1.318220615386963\n",
      "Epoch [48/500], Loss: 1.2484601736068726, Validation Loss: 1.3178130388259888\n",
      "Epoch [49/500], Loss: 1.2286711931228638, Validation Loss: 1.317386269569397\n",
      "Epoch [50/500], Loss: 1.214582920074463, Validation Loss: 1.3169831037521362\n",
      "Epoch [51/500], Loss: 1.2353636026382446, Validation Loss: 1.3165771961212158\n",
      "Epoch [52/500], Loss: 1.319929599761963, Validation Loss: 1.3162120580673218\n",
      "Epoch [53/500], Loss: 1.1060019731521606, Validation Loss: 1.3157483339309692\n",
      "Epoch [54/500], Loss: 1.2485839128494263, Validation Loss: 1.3153736591339111\n",
      "Epoch [55/500], Loss: 1.1776503324508667, Validation Loss: 1.3149224519729614\n",
      "Epoch [56/500], Loss: 1.3065866231918335, Validation Loss: 1.3145033121109009\n",
      "Epoch [57/500], Loss: 1.2534005641937256, Validation Loss: 1.3141474723815918\n",
      "Epoch [58/500], Loss: 1.1658170223236084, Validation Loss: 1.3136992454528809\n",
      "Epoch [59/500], Loss: 1.1777676343917847, Validation Loss: 1.3132871389389038\n",
      "Epoch [60/500], Loss: 1.2888450622558594, Validation Loss: 1.3129264116287231\n",
      "Epoch [61/500], Loss: 1.257411241531372, Validation Loss: 1.3128825426101685\n",
      "Epoch [62/500], Loss: 1.1675338745117188, Validation Loss: 1.3128416538238525\n",
      "Epoch [63/500], Loss: 1.1502923965454102, Validation Loss: 1.312799096107483\n",
      "Epoch [64/500], Loss: 1.2473926544189453, Validation Loss: 1.3127591609954834\n",
      "Epoch [65/500], Loss: 1.1874728202819824, Validation Loss: 1.312713623046875\n",
      "Epoch [66/500], Loss: 1.2833682298660278, Validation Loss: 1.3126744031906128\n",
      "Epoch [67/500], Loss: 1.231682300567627, Validation Loss: 1.3126376867294312\n",
      "Epoch [68/500], Loss: 1.3546644449234009, Validation Loss: 1.312601923942566\n",
      "Epoch [69/500], Loss: 1.2375105619430542, Validation Loss: 1.3125585317611694\n",
      "Epoch [70/500], Loss: 1.3153536319732666, Validation Loss: 1.3125205039978027\n",
      "Epoch [71/500], Loss: 1.2963733673095703, Validation Loss: 1.3124780654907227\n",
      "Epoch [72/500], Loss: 1.1125200986862183, Validation Loss: 1.3124308586120605\n",
      "Epoch [73/500], Loss: 1.1842520236968994, Validation Loss: 1.3123914003372192\n",
      "Epoch [74/500], Loss: 1.273146390914917, Validation Loss: 1.312347650527954\n",
      "Epoch [75/500], Loss: 1.303621530532837, Validation Loss: 1.3123060464859009\n",
      "Epoch [76/500], Loss: 1.2548755407333374, Validation Loss: 1.3122690916061401\n",
      "Epoch [77/500], Loss: 1.1764415502548218, Validation Loss: 1.3122241497039795\n",
      "Epoch [78/500], Loss: 1.1028374433517456, Validation Loss: 1.312180995941162\n",
      "Epoch [79/500], Loss: 1.1726534366607666, Validation Loss: 1.3121448755264282\n",
      "Epoch [80/500], Loss: 1.2198847532272339, Validation Loss: 1.3120951652526855\n",
      "Epoch [81/500], Loss: 1.1910450458526611, Validation Loss: 1.3120535612106323\n",
      "Epoch [82/500], Loss: 1.1687655448913574, Validation Loss: 1.3120191097259521\n",
      "Epoch [83/500], Loss: 1.188538670539856, Validation Loss: 1.3119772672653198\n",
      "Epoch [84/500], Loss: 1.295164942741394, Validation Loss: 1.3119370937347412\n",
      "Epoch [85/500], Loss: 1.2768783569335938, Validation Loss: 1.3119012117385864\n",
      "Epoch [86/500], Loss: 1.2911021709442139, Validation Loss: 1.3118605613708496\n",
      "Epoch [87/500], Loss: 1.216440200805664, Validation Loss: 1.3118126392364502\n",
      "Epoch [88/500], Loss: 1.3044034242630005, Validation Loss: 1.3117725849151611\n",
      "Epoch [89/500], Loss: 1.227067232131958, Validation Loss: 1.311733603477478\n",
      "Epoch [90/500], Loss: 1.2701820135116577, Validation Loss: 1.311694860458374\n",
      "Epoch [91/500], Loss: 1.3011181354522705, Validation Loss: 1.311691164970398\n",
      "Epoch [92/500], Loss: 1.216883659362793, Validation Loss: 1.311686635017395\n",
      "Epoch [93/500], Loss: 1.2084147930145264, Validation Loss: 1.3116823434829712\n",
      "Epoch [94/500], Loss: 1.322462558746338, Validation Loss: 1.3116785287857056\n",
      "Epoch [95/500], Loss: 1.2433946132659912, Validation Loss: 1.3116742372512817\n",
      "Epoch [96/500], Loss: 1.2930898666381836, Validation Loss: 1.3116694688796997\n",
      "Epoch [97/500], Loss: 1.1736818552017212, Validation Loss: 1.311665415763855\n",
      "Epoch [98/500], Loss: 1.2107517719268799, Validation Loss: 1.3116612434387207\n",
      "Epoch [99/500], Loss: 1.269060730934143, Validation Loss: 1.3116573095321655\n",
      "Epoch [100/500], Loss: 1.083906650543213, Validation Loss: 1.3116528987884521\n",
      "Epoch [101/500], Loss: 1.2037737369537354, Validation Loss: 1.3116487264633179\n",
      "Epoch [102/500], Loss: 1.149208903312683, Validation Loss: 1.3116446733474731\n",
      "Epoch [103/500], Loss: 1.1848580837249756, Validation Loss: 1.311640977859497\n",
      "Epoch [104/500], Loss: 1.252920389175415, Validation Loss: 1.3116363286972046\n",
      "Epoch [105/500], Loss: 1.3007692098617554, Validation Loss: 1.311631679534912\n",
      "Epoch [106/500], Loss: 1.2551170587539673, Validation Loss: 1.3116282224655151\n",
      "Epoch [107/500], Loss: 1.1808147430419922, Validation Loss: 1.3116239309310913\n",
      "Epoch [108/500], Loss: 1.2830299139022827, Validation Loss: 1.3116191625595093\n",
      "Epoch [109/500], Loss: 1.107161283493042, Validation Loss: 1.3116153478622437\n",
      "Epoch [110/500], Loss: 1.236295461654663, Validation Loss: 1.3116114139556885\n",
      "Epoch [111/500], Loss: 1.3182618618011475, Validation Loss: 1.3116066455841064\n",
      "Epoch [112/500], Loss: 1.1719508171081543, Validation Loss: 1.3116028308868408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [113/500], Loss: 1.2716596126556396, Validation Loss: 1.3115986585617065\n",
      "Epoch [114/500], Loss: 1.1241265535354614, Validation Loss: 1.3115941286087036\n",
      "Epoch [115/500], Loss: 1.1180927753448486, Validation Loss: 1.3115898370742798\n",
      "Epoch [116/500], Loss: 1.2539808750152588, Validation Loss: 1.3115864992141724\n",
      "Epoch [117/500], Loss: 1.2137950658798218, Validation Loss: 1.3115826845169067\n",
      "Epoch [118/500], Loss: 1.2231292724609375, Validation Loss: 1.3115780353546143\n",
      "Epoch [119/500], Loss: 1.2209595441818237, Validation Loss: 1.3115733861923218\n",
      "Epoch [120/500], Loss: 1.2307555675506592, Validation Loss: 1.3115696907043457\n",
      "Epoch [121/500], Loss: 1.2659943103790283, Validation Loss: 1.3115692138671875\n",
      "Epoch [122/500], Loss: 1.335159420967102, Validation Loss: 1.3115687370300293\n",
      "Epoch [123/500], Loss: 1.377071738243103, Validation Loss: 1.3115684986114502\n",
      "Epoch [124/500], Loss: 1.169845461845398, Validation Loss: 1.3115681409835815\n",
      "Epoch [125/500], Loss: 1.3019250631332397, Validation Loss: 1.3115676641464233\n",
      "Epoch [126/500], Loss: 1.206096887588501, Validation Loss: 1.3115674257278442\n",
      "Epoch [127/500], Loss: 1.24590003490448, Validation Loss: 1.3115668296813965\n",
      "Epoch [128/500], Loss: 1.2998247146606445, Validation Loss: 1.3115665912628174\n",
      "Epoch [129/500], Loss: 1.1781038045883179, Validation Loss: 1.3115661144256592\n",
      "Epoch [130/500], Loss: 1.1497080326080322, Validation Loss: 1.31156587600708\n",
      "Epoch [131/500], Loss: 1.2812081575393677, Validation Loss: 1.3115652799606323\n",
      "Epoch [132/500], Loss: 1.2271443605422974, Validation Loss: 1.3115649223327637\n",
      "Epoch [133/500], Loss: 1.168228268623352, Validation Loss: 1.311564564704895\n",
      "Epoch [134/500], Loss: 1.1934133768081665, Validation Loss: 1.3115640878677368\n",
      "Epoch [135/500], Loss: 1.266728401184082, Validation Loss: 1.3115636110305786\n",
      "Epoch [136/500], Loss: 1.178765058517456, Validation Loss: 1.3115633726119995\n",
      "Epoch [137/500], Loss: 1.3208463191986084, Validation Loss: 1.3115630149841309\n",
      "Epoch [138/500], Loss: 1.2424216270446777, Validation Loss: 1.3115626573562622\n",
      "Epoch [139/500], Loss: 1.278641939163208, Validation Loss: 1.3115622997283936\n",
      "Epoch [140/500], Loss: 1.3204964399337769, Validation Loss: 1.3115618228912354\n",
      "Epoch [141/500], Loss: 1.28240966796875, Validation Loss: 1.3115614652633667\n",
      "Epoch [142/500], Loss: 1.1896142959594727, Validation Loss: 1.311561107635498\n",
      "Epoch [143/500], Loss: 1.2241880893707275, Validation Loss: 1.3115606307983398\n",
      "Epoch [144/500], Loss: 1.2530534267425537, Validation Loss: 1.3115602731704712\n",
      "Epoch [145/500], Loss: 1.217292308807373, Validation Loss: 1.3115599155426025\n",
      "Epoch [146/500], Loss: 1.2396987676620483, Validation Loss: 1.3115595579147339\n",
      "Epoch [147/500], Loss: 1.2179707288742065, Validation Loss: 1.3115592002868652\n",
      "Epoch [148/500], Loss: 1.2416473627090454, Validation Loss: 1.311558723449707\n",
      "Epoch [149/500], Loss: 1.1668099164962769, Validation Loss: 1.3115583658218384\n",
      "Epoch [150/500], Loss: 1.1382076740264893, Validation Loss: 1.3115578889846802\n",
      "Epoch [151/500], Loss: 1.2580664157867432, Validation Loss: 1.3115578889846802\n",
      "Epoch [152/500], Loss: 1.184174656867981, Validation Loss: 1.3115578889846802\n",
      "Epoch [153/500], Loss: 1.242070198059082, Validation Loss: 1.3115578889846802\n",
      "Epoch [154/500], Loss: 1.1872327327728271, Validation Loss: 1.3115578889846802\n",
      "Epoch [155/500], Loss: 1.208028793334961, Validation Loss: 1.3115578889846802\n",
      "Epoch [156/500], Loss: 1.1712167263031006, Validation Loss: 1.3115578889846802\n",
      "Epoch [157/500], Loss: 1.1842856407165527, Validation Loss: 1.3115578889846802\n",
      "Epoch [158/500], Loss: 1.1888148784637451, Validation Loss: 1.3115578889846802\n",
      "Epoch [159/500], Loss: 1.201351523399353, Validation Loss: 1.3115578889846802\n",
      "Epoch [160/500], Loss: 1.176985263824463, Validation Loss: 1.3115578889846802\n",
      "Epoch [161/500], Loss: 1.1855227947235107, Validation Loss: 1.3115578889846802\n",
      "Epoch [162/500], Loss: 1.218059778213501, Validation Loss: 1.3115578889846802\n",
      "Epoch [163/500], Loss: 1.2197142839431763, Validation Loss: 1.3115577697753906\n",
      "Epoch [164/500], Loss: 1.1563591957092285, Validation Loss: 1.3115577697753906\n",
      "Epoch [165/500], Loss: 1.2748770713806152, Validation Loss: 1.3115577697753906\n",
      "Epoch [166/500], Loss: 1.1849861145019531, Validation Loss: 1.3115577697753906\n",
      "Epoch [167/500], Loss: 1.1461455821990967, Validation Loss: 1.3115577697753906\n",
      "Epoch [168/500], Loss: 1.2050813436508179, Validation Loss: 1.311557650566101\n",
      "Epoch [169/500], Loss: 1.3240766525268555, Validation Loss: 1.311557650566101\n",
      "Epoch [170/500], Loss: 1.1577818393707275, Validation Loss: 1.311557650566101\n",
      "Epoch [171/500], Loss: 1.203676700592041, Validation Loss: 1.311557650566101\n",
      "Epoch [172/500], Loss: 1.2054173946380615, Validation Loss: 1.311557650566101\n",
      "Epoch [173/500], Loss: 1.2518603801727295, Validation Loss: 1.311557650566101\n",
      "Epoch [174/500], Loss: 1.234967589378357, Validation Loss: 1.311557650566101\n",
      "Epoch [175/500], Loss: 1.2518789768218994, Validation Loss: 1.311557650566101\n",
      "Epoch [176/500], Loss: 1.1688063144683838, Validation Loss: 1.311557650566101\n",
      "Epoch [177/500], Loss: 1.206882357597351, Validation Loss: 1.311557650566101\n",
      "Epoch [178/500], Loss: 1.299308180809021, Validation Loss: 1.311557650566101\n",
      "Epoch [179/500], Loss: 1.2032020092010498, Validation Loss: 1.311557650566101\n",
      "Epoch [180/500], Loss: 1.1857316493988037, Validation Loss: 1.311557650566101\n",
      "Epoch [181/500], Loss: 1.16508150100708, Validation Loss: 1.311557650566101\n",
      "Epoch [182/500], Loss: 1.2802952527999878, Validation Loss: 1.311557650566101\n",
      "Epoch [183/500], Loss: 1.1762429475784302, Validation Loss: 1.311557650566101\n",
      "Epoch [184/500], Loss: 1.2874257564544678, Validation Loss: 1.311557650566101\n",
      "Epoch [185/500], Loss: 1.2439848184585571, Validation Loss: 1.311557650566101\n",
      "Epoch [186/500], Loss: 1.19574773311615, Validation Loss: 1.311557650566101\n",
      "Epoch [187/500], Loss: 1.3672846555709839, Validation Loss: 1.311557650566101\n",
      "Epoch [188/500], Loss: 1.2290966510772705, Validation Loss: 1.311557650566101\n",
      "Epoch [189/500], Loss: 1.184306263923645, Validation Loss: 1.311557650566101\n",
      "Epoch [190/500], Loss: 1.2005916833877563, Validation Loss: 1.311557650566101\n",
      "Epoch [191/500], Loss: 1.3078349828720093, Validation Loss: 1.311557650566101\n",
      "Epoch [192/500], Loss: 1.215823769569397, Validation Loss: 1.311557650566101\n",
      "Epoch [193/500], Loss: 1.2817169427871704, Validation Loss: 1.311557650566101\n",
      "Epoch [194/500], Loss: 1.3819513320922852, Validation Loss: 1.311557650566101\n",
      "Epoch [195/500], Loss: 1.3447104692459106, Validation Loss: 1.311557650566101\n",
      "Epoch [196/500], Loss: 1.23798406124115, Validation Loss: 1.311557650566101\n",
      "Epoch [197/500], Loss: 1.1522749662399292, Validation Loss: 1.311557650566101\n",
      "Epoch [198/500], Loss: 1.2089680433273315, Validation Loss: 1.311557650566101\n",
      "Epoch [199/500], Loss: 1.2463347911834717, Validation Loss: 1.311557650566101\n",
      "Epoch [200/500], Loss: 1.187354326248169, Validation Loss: 1.311557650566101\n",
      "Epoch [201/500], Loss: 1.2661305665969849, Validation Loss: 1.311557650566101\n",
      "Epoch [202/500], Loss: 1.2401846647262573, Validation Loss: 1.311557650566101\n",
      "Epoch [203/500], Loss: 1.1511601209640503, Validation Loss: 1.311557650566101\n",
      "Epoch [204/500], Loss: 1.214868187904358, Validation Loss: 1.311557650566101\n",
      "Epoch [205/500], Loss: 1.262357473373413, Validation Loss: 1.311557650566101\n",
      "Epoch [206/500], Loss: 1.165282964706421, Validation Loss: 1.311557650566101\n",
      "Epoch [207/500], Loss: 1.2627278566360474, Validation Loss: 1.311557650566101\n",
      "Epoch [208/500], Loss: 1.2179118394851685, Validation Loss: 1.311557650566101\n",
      "Epoch [209/500], Loss: 1.1904364824295044, Validation Loss: 1.311557650566101\n",
      "Epoch [210/500], Loss: 1.3355368375778198, Validation Loss: 1.311557650566101\n",
      "Epoch [211/500], Loss: 1.255333423614502, Validation Loss: 1.311557650566101\n",
      "Epoch [212/500], Loss: 1.3197590112686157, Validation Loss: 1.311557650566101\n",
      "Epoch [213/500], Loss: 1.3109904527664185, Validation Loss: 1.311557650566101\n",
      "Epoch [214/500], Loss: 1.193778157234192, Validation Loss: 1.311557650566101\n",
      "Epoch [215/500], Loss: 1.3163552284240723, Validation Loss: 1.311557650566101\n",
      "Epoch [216/500], Loss: 1.198337435722351, Validation Loss: 1.311557650566101\n",
      "Epoch [217/500], Loss: 1.1993865966796875, Validation Loss: 1.311557650566101\n",
      "Epoch [218/500], Loss: 1.225022554397583, Validation Loss: 1.311557650566101\n",
      "Epoch [219/500], Loss: 1.249045491218567, Validation Loss: 1.311557650566101\n",
      "Early stopping triggered\n",
      "Accuracy over 4 classes: 0.4318181818181818\n",
      "F1 Score over 4 classes: 0.34044965786901266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/l92180p93y13zhldz_14cwpc0000gn/T/ipykernel_1576/3884871855.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions = model(torch.tensor(test_features, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "### Use Bert base model and MLPClassifer for 4 classes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Custom MLP Classifier model in PyTorch\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=4):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(512, 128)        # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, num_classes) # Output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here, CrossEntropyLoss will take care of that\n",
    "        return x\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Setup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.long)\n",
    "\n",
    "# Convert to PyTorch datasets for multi-class\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels_tensor)\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels_tensor)\n",
    "\n",
    "# MLP Classifier Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = MLPClassifier(input_dim)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "step_size = 30 # Number of epochs after which to reduce learning rate\n",
    "gamma = 0.1 # Reduction factor for learning rate\n",
    "\n",
    "# DataLoader with batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Logistic Regression Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_features, val_labels = test_dataset.tensors\n",
    "        val_outputs = model(val_features)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/500], Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break       \n",
    "\n",
    "# Evaluate the model for multi-class classification\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(test_features, dtype=torch.float32))\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    accuracy = accuracy_score(test_labels, predicted_classes)\n",
    "    f1 = f1_score(test_labels, predicted_classes, average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 4 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 4 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cccadc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
