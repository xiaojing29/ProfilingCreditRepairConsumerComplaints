{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cc1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecef775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()  # Extract the [CLS] token's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f07919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = 'complaints-official-2-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae190e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c10666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87750452",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3101f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "log_reg_model = LogisticRegression(max_iter=1000)\n",
    "log_reg_model.fit(train_features, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = log_reg_model.predict(test_features)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Bert and SK-Learn Logistic Regression for 2 classes with grid search\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()  # Extract the [CLS] token's embeddings\n",
    "\n",
    "# Load pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'complaints-official-2-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, model)\n",
    "\n",
    "# Logistic Regression Classifier with Grid Search\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'newton-cg', 'sag', 'saga']\n",
    "}\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_model.predict(test_features)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_2_classes = accuracy_score(test_labels, predictions)\n",
    "f1_2_classes = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Accuracy over 2 classes: {accuracy_2_classes}\")\n",
    "print(f\"F1 Score over 2 classes: {f1_2_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7682d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using Bert and SK-Learn Logistic Regression for 4 classes with grid search\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()  # Extract the [CLS] token's embeddings\n",
    "\n",
    "# Load pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split dataset into training and testing\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, model)\n",
    "\n",
    "# Logistic Regression Classifier with Grid Search\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'newton-cg', 'sag', 'saga']\n",
    "}\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_model.predict(test_features)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_4_classes = accuracy_score(test_labels, predictions)\n",
    "f1_4_classes = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Accuracy over 4 classes: {accuracy_4_classes}\")\n",
    "print(f\"F1 Score over 4 classes: {f1_4_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57455f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of classes\n",
    "classes = [2, 4]\n",
    "\n",
    "# Accuracies and F1 scores\n",
    "accuracies = [accuracy_2_classes, accuracy_4_classes]\n",
    "f1_scores = [f1_2_classes, f1_4_classes]\n",
    "\n",
    "# Plotting Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(classes, accuracies, marker='o')\n",
    "plt.title('Accuracy over Number of Classes')\n",
    "plt.xlabel('Number of Classes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(classes)\n",
    "\n",
    "# Plotting F1 Score\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(classes, f1_scores, marker='o', color='orange')\n",
    "plt.title('F1 Score over Number of Classes')\n",
    "plt.xlabel('Number of Classes')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(classes)\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Bert base model and logistic regression in PyTorch with manual hyperparameter tuning (2 classes)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom Logistic Regression model in PyTorch\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Setup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-2-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), torch.tensor(train_labels.values, dtype=torch.float32).view(-1, 1))\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), torch.tensor(test_labels.values, dtype=torch.float32).view(-1, 1))\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "step_size = 30 # Number of epochs after which to reduce learning rate\n",
    "gamma = 0.1 # Reduction factor for learning rate\n",
    "\n",
    "# DataLoader with batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Logistic Regression Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features, labels\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features).view(-1)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        loss = criterion(outputs, labels.view(-1))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    with torch.no_grad():\n",
    "        val_features, val_labels = test_dataset.tensors\n",
    "        val_features, val_labels = val_features, val_labels\n",
    "        val_outputs = model(val_features).view(-1)\n",
    "        val_outputs = torch.sigmoid(val_outputs)\n",
    "        val_loss = criterion(val_outputs, val_labels.view(-1))\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/100], Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break       \n",
    "        \n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(test_features, dtype=torch.float32)).sigmoid().round()\n",
    "    accuracy = accuracy_score(test_labels, predictions.cpu().numpy())\n",
    "    f1 = f1_score(test_labels, predictions.cpu().numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 2 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 2 classes: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Bert base model and logistic regression in PyTorch with manual hyperparameter tuning (4 classes)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Custom Logistic Regression model in PyTorch\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=4):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Setup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert labels for multi-class classification\n",
    "train_labels = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels.values, dtype=torch.long)\n",
    "\n",
    "# Convert to PyTorch datasets for multi-class\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels)\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "step_size = 30 # Number of epochs after which to reduce learning rate\n",
    "gamma = 0.1 # Reduction factor for learning rate\n",
    "\n",
    "# DataLoader with batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Logistic Regression Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_features, val_labels = test_dataset.tensors\n",
    "        val_outputs = model(val_features)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/500], Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break       \n",
    "\n",
    "# Evaluate the model for multi-class classification\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(test_features, dtype=torch.float32))\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    accuracy = accuracy_score(test_labels.cpu().numpy(), predicted_classes.cpu().numpy())\n",
    "    f1 = f1_score(test_labels.cpu().numpy(), predicted_classes.cpu().numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 4 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 4 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e73dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use Bert base model and MLPClassifer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Custom MLP Classifier model in PyTorch\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=4):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(512, 128)        # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, num_classes) # Output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation function here, CrossEntropyLoss will take care of that\n",
    "        return x\n",
    "\n",
    "# Function to extract features using BERT\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Setup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.long)\n",
    "\n",
    "# Convert to PyTorch datasets for multi-class\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), train_labels_tensor)\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), test_labels_tensor)\n",
    "\n",
    "# MLP Classifier Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = MLPClassifier(input_dim)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "batch_size = 16\n",
    "step_size = 30 # Number of epochs after which to reduce learning rate\n",
    "gamma = 0.1 # Reduction factor for learning rate\n",
    "\n",
    "# DataLoader with batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Logistic Regression Model\n",
    "input_dim = train_features.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    model.train()\n",
    "    for features, labels in train_loader:\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_features, val_labels = test_dataset.tensors\n",
    "        val_outputs = model(val_features)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/500], Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break       \n",
    "\n",
    "# Evaluate the model for multi-class classification\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(torch.tensor(test_features, dtype=torch.float32))\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    accuracy = accuracy_score(test_labels, predicted_classes)\n",
    "    f1 = f1_score(test_labels, predicted_classes, average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 4 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 4 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentence Transformer with MLPClassifier for 4 classes\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'complaints-official-4-classes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df = df[['Consumer complaint narrative', 'Label']]\n",
    "df.dropna(inplace=True)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Consumer complaint narrative'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Setup for stsb-bert-base model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Extract features\n",
    "def extract_features(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "train_features = extract_features(train_texts.tolist(), tokenizer, bert_model)\n",
    "test_features = extract_features(test_texts.tolist(), tokenizer, bert_model)\n",
    "\n",
    "# Convert labels for multi-class classification\n",
    "train_labels = train_labels.values\n",
    "test_labels = test_labels.values\n",
    "\n",
    "# Define MLPClassifier\n",
    "mlp_classifier = MLPClassifier()\n",
    "\n",
    "# Define hyperparameters grid for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 3000, 6000]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(mlp_classifier, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train MLPClassifier with best hyperparameters\n",
    "best_mlp_classifier = MLPClassifier(**best_params)\n",
    "best_mlp_classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Predict using best model\n",
    "predictions = best_mlp_classifier.predict(test_features)\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy over 4 classes: {accuracy}\")\n",
    "print(f\"F1 Score over 4 classes: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad27f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
